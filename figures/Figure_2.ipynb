{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from eval import get_run_metrics, read_run_dir, get_model_from_run\n",
    "from samplers import get_data_sampler, sample_transformation\n",
    "from tasks import get_task_sampler\n",
    "from models import LeastSquaresModel\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sns.set_theme('notebook', 'whitegrid')\n",
    "palette = sns.color_palette('colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = \"/Users/harmonbhasin/hsbhasin@wisc.edu - Google Drive/My Drive/ICL/Models\"\n",
    "df = read_run_dir(run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(subset_models, task_args, data_args, seed, att_mask=None):\n",
    "    models = {}\n",
    "    configs = {}\n",
    "    n_points = 100\n",
    "    n_dims = 20\n",
    "    b_size = 64\n",
    "\n",
    "    for key in subset_models:\n",
    "        task = df.task[subset_models[key]]\n",
    "        run_id = df.run_id[subset_models[key]]\n",
    "        run_path = os.path.join(run_dir, task, run_id)\n",
    "        models[key], configs[key] = get_model_from_run(run_path)\n",
    "\n",
    "    preDict = {}\n",
    "    yDict = {}\n",
    "\n",
    "    for task_key in task_args:\n",
    "        current_task = task_args[task_key]\n",
    "        task_sampler = get_task_sampler(\n",
    "            current_task['task'],\n",
    "            n_dims,\n",
    "            b_size,\n",
    "            **{i: current_task[i] for i in current_task if i != 'task'}\n",
    "        )\n",
    "        print('Conducting evaluation on', current_task['task'], 'with degree of ', current_task['degree'], '\\n')\n",
    "        for data_key in data_args:\n",
    "            torch.manual_seed(seed)\n",
    "            current_data = data_args[data_key]\n",
    "            sample_args = {}\n",
    "\n",
    "            if current_data['type'] == 'Skewed':\n",
    "                eigenvals = 1 / (torch.arange(n_dims) + 1)\n",
    "                scale = sample_transformation(eigenvals, normalize=True)\n",
    "                sample_args['scale'] = scale\n",
    "\n",
    "            for data_info in current_data:\n",
    "                if data_info != 'data' and data_info != 'type':\n",
    "                    sample_args[data_info] = current_data[data_info]\n",
    "\n",
    "            data_sampler = get_data_sampler(current_data['data'], n_dims, **sample_args)\n",
    "\n",
    "            # Create an instance of the function class sampler; add seed stuff here\n",
    "            task = task_sampler()\n",
    "\n",
    "            # Generate x points; add seed stuff here\n",
    "            xs = data_sampler.sample_xs(n_points, b_size)\n",
    "\n",
    "            # Generate y points\n",
    "            ys = task.evaluate(xs)\n",
    "\n",
    "            yDict[f\"{task_key}_{data_key}\"] = ys\n",
    "\n",
    "            for model_key in models:\n",
    "                prompt_col = 0\n",
    "                prompt_row = 0\n",
    "                conf = configs[model_key]['training']\n",
    "                prompt_args = conf['prompt_kwargs']\n",
    "                if len(prompt_args) != 0:\n",
    "                       if prompt_args['type'] == 'data':\n",
    "                           if prompt_args['encoding'] == 'dynamic':\n",
    "                               prompt_row = int(data_key[-1])\n",
    "                           prompt_col = prompt_args['position']\n",
    "                       elif prompt_args['type'] == 'task':\n",
    "                           if prompt_args['encoding'] == 'dynamic':\n",
    "                               prompt_row = int(task_key[-1])\n",
    "                           prompt_col = prompt_args['position']\n",
    "\n",
    "                temp_key = f\"{model_key}_{task_key}_{data_key}\"\n",
    "                with torch.no_grad():\n",
    "                    if att_mask is None:\n",
    "                        _, _, preDict[temp_key] = models[model_key](xs,\n",
    "                                                            ys,\n",
    "                                                            conf['prompt_type'],\n",
    "                                                            prompt_row,\n",
    "                                                            prompt_col)\n",
    "                    else:\n",
    "                        _, _, preDict[temp_key] = models[model_key](xs,\n",
    "                                                            ys,\n",
    "                                                            conf['prompt_type'],\n",
    "                                                            prompt_row,\n",
    "                                                            prompt_col,\n",
    "                                                            att_mask=att_mask)\n",
    "\n",
    "                    if conf['prompt_type'] == 'standard':\n",
    "                        print(configs[model_key]['wandb']['name'], 'evaluated on', data_args[data_key]['type'], data_args[data_key]['data'], 'distribution')\n",
    "                    else:\n",
    "                        print(configs[model_key]['wandb']['name'], 'evaluated on', data_args[data_key]['type'], data_args[data_key]['data'], 'distribution with instruction in row', prompt_row, 'and column', prompt_col)\n",
    "            OLS = LeastSquaresModel()\n",
    "            preDict[f'OLS_{task_key}_{data_key}'] = OLS(xs, ys)\n",
    "\n",
    "    lossDict = {}\n",
    "    for pred_key in preDict:\n",
    "        for y_key in yDict:\n",
    "            if y_key in pred_key:\n",
    "                lossDict[pred_key] = (yDict[y_key] - preDict[pred_key]).square().numpy()\n",
    "\n",
    "    different_plots = {}\n",
    "\n",
    "    for key in lossDict:\n",
    "        components = key.split('_')\n",
    "        model = components[0]\n",
    "        task = components[1]\n",
    "        data = components[2]\n",
    "        task_data = f\"{task}_{data}\"\n",
    "        if task_data not in different_plots:\n",
    "            different_plots[task_data] = {}\n",
    "        different_plots[task_data][model] = lossDict[key]\n",
    "\n",
    "    return different_plots, configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "def format_y_axis(value, _):\n",
    "    return f'{value:.2e}'  # Format in scientific notation with two decimal places\n",
    "\n",
    "\n",
    "def plot_results_horizontal_individual(different_plots, configs,task_args, data_args, type, head_dir, smooth, window_size = 10, limit_y=False, y_lim_low=[-0.1,-0.1, -0.1], y_lim_high=[1.0,1.0,1.0], width=12,height=4, legend=False, include_title=False, grid=False, scinot=False, force_ticks=False, format_two=False, name=None):\n",
    "    sns.set_palette(\"Set3\")\n",
    "    color_index = 0\n",
    "    for key in configs:\n",
    "        if key != 'OLS':\n",
    "            if 'Mixed' in configs[key]['wandb']['name']:\n",
    "                configs[key]['name'] = 'Mixed model'\n",
    "            elif 'Random' in configs[key]['wandb']['name']:\n",
    "                configs[key]['name'] = 'Random model'\n",
    "            elif 'Sequential' in configs[key]['wandb']['name']:\n",
    "                configs[key]['name'] = 'Sequential model'\n",
    "            elif 'Skewed Gaussian' in configs[key]['wandb']['name']:\n",
    "                configs[key]['name'] = 'Skewed gaussian model'\n",
    "            elif 'Gaussian' in configs[key]['wandb']['name'] and type == 'task':\n",
    "                configs[key]['name'] = 'Linear model'\n",
    "            elif 'Gaussian' in configs[key]['wandb']['name'] and type == 'sample':\n",
    "                configs[key]['name'] = 'Gaussian model'\n",
    "            elif 'Binomial' in configs[key]['wandb']['name']:\n",
    "                configs[key]['name'] = 'Quadratic model'\n",
    "            elif 'Trinomial' in configs[key]['wandb']['name']:\n",
    "                configs[key]['name'] = 'Cubic model'\n",
    "            elif 'Student-T' in configs[key]['wandb']['name']:\n",
    "                configs[key]['name'] = 'Student t model'\n",
    "          \n",
    "\n",
    "\n",
    "            if configs[key]['training']['prompt_type'] != 'standard':\n",
    "                configs[key]['ls'] = 'dashed'\n",
    "            else:\n",
    "                configs[key]['ls'] = 'solid'\n",
    "            configs[key]['c'] = sns.color_palette()[color_index]\n",
    "            color_index +=1\n",
    "\n",
    "    configs['OLS'] = {}\n",
    "    configs['OLS']['name'] = 'Least Squares'\n",
    "    configs['OLS']['ls'] = 'dashdot'\n",
    "    configs['OLS']['c'] = 'black'\n",
    "\n",
    "    num_cols = len(different_plots)\n",
    "\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "    model_dict = {}\n",
    "    for index, task_data in enumerate(different_plots):\n",
    "        for model in different_plots[task_data]:\n",
    "            if model not in model_dict:\n",
    "                _, model_dict[model] = plt.subplots(nrows=1, ncols=num_cols, figsize=(width,height), tight_layout=True)\n",
    "            \n",
    "            if smooth:\n",
    "                    # Moving average via ChatGPT\n",
    "                    smoothed_data = np.convolve(different_plots[task_data][model].mean(axis=0)/20, np.ones((window_size,))/window_size, mode='valid')\n",
    "                    model_dict[model][index].plot(smoothed_data,\n",
    "                                    lw=2,\n",
    "                                    label=f\"{configs[model]['name']} (MA)\",\n",
    "                                    color='#d95f02')\n",
    "                    model_dict[model][index].plot(different_plots[task_data][model].mean(axis=0)/20,\n",
    "                                    lw=2,\n",
    "                                    label=configs[model]['name'],\n",
    "                                    color='#7570b3',\n",
    "                                    ls='dotted')\n",
    "                    #model_dict[model][index].set_title(f\"{configs[model]['name']}\")\n",
    "    for key in model_dict:\n",
    "        if grid == False:\n",
    "            model_dict[key][2].legend(prop={'family': 'Times New Roman', 'size': 10}, loc='upper right', frameon=True)\n",
    "        model_dict[key][0].set_ylabel(\"Normalized MSE\", fontname='Times New Roman', fontsize=12)\n",
    "        for ax in model_dict[key]:\n",
    "            ax.set_ylim(0)\n",
    "            ax.xaxis.set_major_locator(plt.MultipleLocator(10))\n",
    "            #ax.figure.patch.set_facecolor('white')\n",
    "            #ax.legend(prop={'family': 'Times New Roman', 'size': 12})\n",
    "            #ax.yaxis.set_major_locator(plt.MaxNLocator(nbins=4, prune='both'))\n",
    "            #ax.tick_params(axis='x', rotation=90)\n",
    "            if force_ticks:\n",
    "                ax.yaxis.set_major_locator(MaxNLocator(5))\n",
    "            if format_two:\n",
    "                ax.yaxis.set_major_formatter(FuncFormatter(format_y_axis))\n",
    "            if scinot:\n",
    "                ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "                ax.xaxis.offsetText.set_fontname('Times New Roman')\n",
    "                ax.yaxis.offsetText.set_fontname('Times New Roman')\n",
    "            for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "                label.set_fontname('Times New Roman')\n",
    "                label.set_fontsize(12)\n",
    "            if grid == True:\n",
    "                ax.legend(prop={'family': 'Times New Roman', 'size': 10}, loc='upper right', frameon=True)\n",
    "           \n",
    "            ax.set_xlabel(\"# in-context examples\", fontname='Times New Roman', fontsize=12)\n",
    "        if name == None:\n",
    "            plt.savefig(f\"{head_dir}/eval_{configs[key]['wandb']['name']}.pdf\")\n",
    "        else:\n",
    "            plt.savefig(f\"{head_dir}/{name}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "model_selection = {}\n",
    "model_to_name = {}\n",
    "#if \"model_14\" in task_name and (\"binomial\" in task_name or \"mixed\" in task_name) and \"_i_\" not in task_name:\n",
    "#if (\"model_12\" in task_name or \"model_11_gaussian\" in task_name) and \"_i\" not in task_name:\n",
    "#if \"model_12_mixed\" in task_name and '_i' not in task_name:\n",
    "for task_name in df.task:\n",
    "    if \"model_14\" in task_name and (\"binomial\" in task_name or \"mixed\" in task_name) and \"_i_\" not in task_name:\n",
    "        task_index = df.loc[df[\"task\"] == task_name].index[0]\n",
    "        model_selection[f'model{index}'] = task_index\n",
    "        print(f\"model{index}:\", task_name)\n",
    "        model_to_name[f\"model{index}\"] = task_name\n",
    "        index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset_keys = ['model0']\n",
    "#subset_models = {key: model_selection[key] for key in subset_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Always label the following distributions their corresponding keys even if only evaluating on one\n",
    "# task0 - 1st degree\n",
    "# task1 - 2nd degree\n",
    "# task2 - 3rd degree\n",
    "## If you would like to add more distributions, simply add them as task0, task1, task2\n",
    "#             'task3': {'task': \"hermite_regression\", 'degree': 4}}\n",
    "\n",
    "# Model to evaluate against\n",
    "task_args = {'task0': {'task': \"hermite_regression\", 'degree': 1},\n",
    "             'task1': {'task': \"hermite_regression\", 'degree': 2},\n",
    "             'task2': {'task': \"hermite_regression\", 'degree': 3}}\n",
    "#             'task3': {'task': \"hermite_regression\", 'degree': 4}}\n",
    "\n",
    "#task_args = {'task1': {'task': \"hermite_regression\", 'degree': 2}}\n",
    "data_args = {'data0': {'data': 'gaussian', 'type': 'Standard'}}\n",
    "seed=-152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 12\n",
    "num_heads = 8\n",
    "attention_mask = torch.ones((num_layers, num_heads))\n",
    "# Ignoring important attention heads (layer, head)\n",
    "final_list_tuple = [(4,1), (4,5), (4,6), (4,8), (5,4), (5,8), (6,7)]\n",
    "#list_tuple = [(6,7)]\n",
    "# list_typle = [(12,8)]\n",
    "#final_list_tuple = [(12,8), (10, 4), (8,7), (1,1), (7,1), (2,7), (3,8)]\n",
    "\n",
    "# Define the ranges for the first and second elements\n",
    "#range_first_element = range(1, 13)  # Example range for the first element (1 to 4)\n",
    "#range_second_element = range(1, 9)  # Example range for the second element (2 to 5)\n",
    "\n",
    "# Generate all possible tuples within the specified ranges\n",
    "#all_possible_tuples = [(i, j) for i in range_first_element for j in range_second_element]\n",
    "\n",
    "# Filter out tuples that exist in the 'existing_tuples' list\n",
    "#non_existing_tuples = [t for t in all_possible_tuples if t not in list_tuple]\n",
    "\n",
    "for layer_head in final_list_tuple:\n",
    "    attention_mask[(layer_head[0]-1), (layer_head[1]-1)] = 0\n",
    "\n",
    "#for layer_head in non_existing_tuples:\n",
    "#    attention_mask[(layer_head[0]-1), (layer_head[1]-1)] = 0\n",
    "\n",
    "\n",
    "attention_mask = attention_mask.type(torch.ByteTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_plots, configs = run_evaluation(subset_models=model_selection,\n",
    "                   task_args=task_args,\n",
    "                   data_args=data_args,\n",
    "                   seed=seed)\n",
    "                   #att_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all available models\n",
    "#for key in different_plots['task0_data0']:\n",
    "for key in different_plots['task1_data0']:\n",
    "    if key == 'OLS':\n",
    "        print('OLS')\n",
    "    else:\n",
    "        print(f\"{key} = {model_to_name[key]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_models = True # Set this true if you only want to plot some of the models that you evaluated on\n",
    "#models_you_want = ['model0', 'model1', 'model3', 'model4', 'model5', 'model6']\n",
    "models_you_want = ['model0', 'model1', 'model2', 'model3', 'model4', 'model5', 'model6', 'model7', 'model8', 'model9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select models specified in previous cell\n",
    "eval_calculations={}\n",
    "changeable_config={}\n",
    "\n",
    "if select_models:\n",
    "    for key in copy.deepcopy(different_plots):\n",
    "        current_task_data = different_plots[key]\n",
    "        eval_calculations[key] = {second_key: current_task_data[second_key] for second_key in models_you_want}\n",
    "            \n",
    "    for config_key in models_you_want:\n",
    "        if config_key != 'OLS':\n",
    "            changeable_config[config_key] = copy.deepcopy(configs)[config_key]\n",
    "else:\n",
    "    eval_calculations = copy.deepcopy(different_plots)\n",
    "    changeable_config = copy.deepcopy(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(eval_calculations['task0_data0'].keys())\n",
    "print(eval_calculations['task1_data0'].keys())\n",
    "print(changeable_config.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_dir=\"/Users/harmonbhasin/College/Research/huLab/paper_figures/task_mask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_lim_low=[0, 0, 0, 0]\n",
    "#y_lim_high=[1, 5, 10, 0.4]\n",
    "\n",
    "for target_key in models_you_want:\n",
    "    subdict={}\n",
    "    for outer_key, inner_dict in eval_calculations.items():\n",
    "        if target_key in inner_dict:\n",
    "            subdict[outer_key] = {target_key : inner_dict[target_key]}\n",
    "    plot_results_horizontal_individual(different_plots=subdict,\n",
    "                configs=changeable_config,\n",
    "                task_args=task_args,\n",
    "                data_args=data_args,\n",
    "                type='task',\n",
    "                limit_y=False,\n",
    "                head_dir=head_dir,\n",
    "                smooth=True,\n",
    "                width=9,\n",
    "                height=3,\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
