{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from eval import get_run_metrics, read_run_dir, get_model_from_run\n",
    "from samplers import get_data_sampler, sample_transformation\n",
    "from tasks import get_task_sampler\n",
    "from models import LeastSquaresModel\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sns.set_theme('notebook', 'whitegrid')\n",
    "palette = sns.color_palette('colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = \"/Users/harmonbhasin/hsbhasin@wisc.edu - Google Drive/My Drive/ICL/Models\"\n",
    "df = read_run_dir(run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(subset_models, task_args, data_args, seed, att_mask=None):\n",
    "    models = {}\n",
    "    configs = {}\n",
    "    n_points = 100\n",
    "    n_dims = 20\n",
    "    b_size = 64\n",
    "\n",
    "    for key in subset_models:\n",
    "        task = df.task[subset_models[key]]\n",
    "        run_id = df.run_id[subset_models[key]]\n",
    "        run_path = os.path.join(run_dir, task, run_id)\n",
    "        models[key], configs[key] = get_model_from_run(run_path)\n",
    "\n",
    "    preDict = {}\n",
    "    yDict = {}\n",
    "\n",
    "    for task_key in task_args:\n",
    "        current_task = task_args[task_key]\n",
    "        task_sampler = get_task_sampler(\n",
    "            current_task['task'],\n",
    "            n_dims,\n",
    "            b_size,\n",
    "            **{i: current_task[i] for i in current_task if i != 'task'}\n",
    "        )\n",
    "        print('Conducting evaluation on', current_task['task'], 'with degree of ', current_task['degree'], '\\n')\n",
    "        for data_key in data_args:\n",
    "            torch.manual_seed(seed)\n",
    "            current_data = data_args[data_key]\n",
    "            sample_args = {}\n",
    "\n",
    "            if current_data['type'] == 'Skewed':\n",
    "                eigenvals = 1 / (torch.arange(n_dims) + 1)\n",
    "                scale = sample_transformation(eigenvals, normalize=True)\n",
    "                sample_args['scale'] = scale\n",
    "\n",
    "            for data_info in current_data:\n",
    "                if data_info != 'data' and data_info != 'type':\n",
    "                    sample_args[data_info] = current_data[data_info]\n",
    "\n",
    "            data_sampler = get_data_sampler(current_data['data'], n_dims, **sample_args)\n",
    "\n",
    "            # Create an instance of the function class sampler; add seed stuff here\n",
    "            task = task_sampler()\n",
    "\n",
    "            # Generate x points; add seed stuff here\n",
    "            xs = data_sampler.sample_xs(n_points, b_size)\n",
    "\n",
    "            # Generate y points\n",
    "            ys = task.evaluate(xs)\n",
    "\n",
    "            yDict[f\"{task_key}_{data_key}\"] = ys\n",
    "\n",
    "            for model_key in models:\n",
    "                prompt_col = 0\n",
    "                prompt_row = 0\n",
    "                conf = configs[model_key]['training']\n",
    "                prompt_args = conf['prompt_kwargs']\n",
    "                if len(prompt_args) != 0:\n",
    "                       if prompt_args['type'] == 'data':\n",
    "                           if prompt_args['encoding'] == 'dynamic':\n",
    "                               prompt_row = int(data_key[-1])\n",
    "                           prompt_col = prompt_args['position']\n",
    "                       elif prompt_args['type'] == 'task':\n",
    "                           if prompt_args['encoding'] == 'dynamic':\n",
    "                               prompt_row = int(task_key[-1])\n",
    "                           prompt_col = prompt_args['position']\n",
    "\n",
    "                temp_key = f\"{model_key}_{task_key}_{data_key}\"\n",
    "                with torch.no_grad():\n",
    "                    if att_mask is None:\n",
    "                        _, _, preDict[temp_key] = models[model_key](xs,\n",
    "                                                            ys,\n",
    "                                                            conf['prompt_type'],\n",
    "                                                            prompt_row,\n",
    "                                                            prompt_col)\n",
    "                    else:\n",
    "                        _, _, preDict[temp_key] = models[model_key](xs,\n",
    "                                                            ys,\n",
    "                                                            conf['prompt_type'],\n",
    "                                                            prompt_row,\n",
    "                                                            prompt_col,\n",
    "                                                            att_mask=att_mask)\n",
    "\n",
    "                    if conf['prompt_type'] == 'standard':\n",
    "                        print(configs[model_key]['wandb']['name'], 'evaluated on', data_args[data_key]['type'], data_args[data_key]['data'], 'distribution')\n",
    "                    else:\n",
    "                        print(configs[model_key]['wandb']['name'], 'evaluated on', data_args[data_key]['type'], data_args[data_key]['data'], 'distribution with instruction in row', prompt_row, 'and column', prompt_col)\n",
    "            OLS = LeastSquaresModel()\n",
    "            preDict[f'OLS_{task_key}_{data_key}'] = OLS(xs, ys)\n",
    "\n",
    "    lossDict = {}\n",
    "    for pred_key in preDict:\n",
    "        for y_key in yDict:\n",
    "            if y_key in pred_key:\n",
    "                lossDict[pred_key] = (yDict[y_key] - preDict[pred_key]).square().numpy()\n",
    "\n",
    "    different_plots = {}\n",
    "\n",
    "    for key in lossDict:\n",
    "        components = key.split('_')\n",
    "        model = components[0]\n",
    "        task = components[1]\n",
    "        data = components[2]\n",
    "        task_data = f\"{task}_{data}\"\n",
    "        if task_data not in different_plots:\n",
    "            different_plots[task_data] = {}\n",
    "        different_plots[task_data][model] = lossDict[key]\n",
    "\n",
    "    return different_plots, configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_one_plot_results_horizontal(different_plots, configs, task_args, data_args, type, head_dir, smooth, window_size = 10, limit_y=False, y_lim_low=[-0.1,-0.1, -0.1], y_lim_high=[1.0,1.0,1.0], width=12,height=4, legend=False, title=None, name = None):\n",
    "    task_num = len(task_args)\n",
    "    data_num = len(data_args)\n",
    "\n",
    "    color_index_mixed = 0\n",
    "    color_index_quadratic = 0\n",
    "    for key in configs:\n",
    "        if key != 'OLS':\n",
    "            if 'Mixed' in configs[key]['wandb']['name']:\n",
    "                configs[key]['name'] = 'Mixed model'\n",
    "                sns.set_palette(\"OrRd\")\n",
    "                configs[key]['c'] = sns.color_palette()[color_index_mixed]\n",
    "                color_index_mixed +=1\n",
    "            else:\n",
    "                configs[key]['name'] = 'Quadratic model'\n",
    "                sns.set_palette(\"BuPu\")\n",
    "                configs[key]['c'] = sns.color_palette()[color_index_quadratic]\n",
    "                color_index_quadratic +=1\n",
    "\n",
    "\n",
    "    configs['OLS'] = {}\n",
    "    configs['OLS']['name'] = 'Least Squares'\n",
    "    configs['OLS']['ls'] = 'dashdot'\n",
    "    configs['OLS']['c'] = 'black'\n",
    "\n",
    "    if type == 'task':\n",
    "        #_, axs = plt.subplots(nrows=1, ncols=task_num, figsize=(width,height), tight_layout=True)\n",
    "        _, axs = plt.subplots(nrows=1, ncols=1, figsize=(width,height))\n",
    "    else:\n",
    "        _, axs = plt.subplots(nrows=1, ncols=data_num, figsize=(width,height), tight_layout=True)\n",
    "\n",
    "    if type == 'task':\n",
    "        #for i in range(task_num):\n",
    "        #    for j in range(data_num):\n",
    "        i=1\n",
    "        j=0\n",
    "        current_task = f\"task{i}\"\n",
    "        current_data = f\"data{j}\"\n",
    "        current_eval = different_plots[f\"{current_task}_{current_data}\"]\n",
    "        for key in current_eval:\n",
    "            if smooth:\n",
    "                # Moving average via ChatGPT\n",
    "                smoothed_data = np.convolve(current_eval[key].mean(axis=0)/20, np.ones((window_size,))/window_size, mode='valid')\n",
    "                axs.plot(smoothed_data,\n",
    "                                lw=1,\n",
    "                                label=f\"{configs[key]['name']} (MA)\",\n",
    "                                color=configs[key]['c'])\n",
    "            else:\n",
    "                axs.plot(current_eval[key].mean(axis=0)/20,\n",
    "                                lw=1,\n",
    "                                label=configs[key]['name'],\n",
    "                                color='black')\n",
    "            if limit_y:\n",
    "                axs.set_ylim(y_lim_low[i,j],y_lim_high[i,j])\n",
    "    else:\n",
    "        for i in range(data_num):\n",
    "            for j in range(task_num):\n",
    "                current_task = f\"task{j}\"\n",
    "                current_data = f\"data{i}\"\n",
    "                current_eval = different_plots[f\"{current_task}_{current_data}\"]\n",
    "                for key in current_eval:\n",
    "                    axs[i].plot(current_eval[key].mean(axis=0),\n",
    "                                    lw=2,\n",
    "                                    label=configs[key]['name'],\n",
    "                                    color='black')\n",
    "\n",
    "                    if limit_y:\n",
    "                        axs[i].set_ylim(y_lim_low[i,j],y_lim_high[i,j])\n",
    "\n",
    "    axs.set_ylabel(\"Normalized MSE\", fontname='Times New Roman', fontsize=10)\n",
    "\n",
    "    axs.xaxis.set_major_locator(plt.MultipleLocator(10))\n",
    "    axs.yaxis.set_major_locator(plt.MultipleLocator(3))\n",
    "    #ax.yaxis.set_major_locator(plt.MaxNLocator(nbins=4, prune='both'))\n",
    "    #ax.tick_params(axis='x', rotation=90)\n",
    "    for label in (axs.get_xticklabels() + axs.get_yticklabels()):\n",
    "        label.set_fontname('Times New Roman')\n",
    "        label.set_fontsize(10)\n",
    "    axs.set_xlabel(\"# in-context examples\", fontname='Times New Roman', fontsize=10)\n",
    "    if legend:\n",
    "        #axs.legend(prop={'family': 'Times New Roman', 'size': 10}, loc='center right', bbox_to_anchor=(1, 2), frameon=True, ncol=2)\n",
    "        axs.legend(prop={'family': 'Times New Roman', 'size': 10}, loc='center right', bbox_to_anchor=(1, 2), frameon=True)\n",
    "\n",
    "    if title != None:\n",
    "        plt.figtext(0.5, 0.925, title, ha='center', fontname='Times New Roman', fontsize=12, fontweight='bold')\n",
    "\n",
    "\n",
    "    if name == None:\n",
    "        plt.savefig(f\"{head_dir}/eval_{configs[key]['wandb']['name']}.pdf\", bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(f\"{head_dir}/{name}.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "model_selection = {}\n",
    "model_to_name = {}\n",
    "for task_name in df.task:\n",
    "    if \"model_14\" in task_name and (\"binomial\" in task_name or \"mixed\" in task_name) and \"_i_\" not in task_name:\n",
    "        task_index = df.loc[df[\"task\"] == task_name].index[0]\n",
    "        model_selection[f'model{index}'] = task_index\n",
    "        print(f\"model{index}:\", task_name)\n",
    "        model_to_name[f\"model{index}\"] = task_name\n",
    "        index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Always label the following distributions their corresponding keys even if only evaluating on one\n",
    "# task0 - 1st degree\n",
    "# task1 - 2nd degree\n",
    "# task2 - 3rd degree\n",
    "## If you would like to add more distributions, simply add them as task0, task1, task2\n",
    "#             'task3': {'task': \"hermite_regression\", 'degree': 4}}\n",
    "\n",
    "# Model to evaluate against\n",
    "task_args = {'task0': {'task': \"hermite_regression\", 'degree': 1},\n",
    "             'task1': {'task': \"hermite_regression\", 'degree': 2},\n",
    "             'task2': {'task': \"hermite_regression\", 'degree': 3}}\n",
    "#             'task3': {'task': \"hermite_regression\", 'degree': 4}}\n",
    "\n",
    "#task_args = {'task1': {'task': \"hermite_regression\", 'degree': 2}}\n",
    "data_args = {'data0': {'data': 'gaussian', 'type': 'Standard'}}\n",
    "seed=-152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_plots, configs = run_evaluation(subset_models=model_selection,\n",
    "                   task_args=task_args,\n",
    "                   data_args=data_args,\n",
    "                   seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all available models\n",
    "#for key in different_plots['task0_data0']:\n",
    "for key in different_plots['task1_data0']:\n",
    "    if key == 'OLS':\n",
    "        print('OLS')\n",
    "    else:\n",
    "        print(f\"{key} = {model_to_name[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_models = True # Set this true if you only want to plot some of the models that you evaluated on\n",
    "#models_you_want = ['model0', 'model1', 'model3', 'model4', 'model5', 'model6']\n",
    "#models_you_want = ['model0', 'model1', 'model2', 'model3', 'model4', 'model5', 'model6', 'model7', 'model8', 'model9']\n",
    "models_you_want = ['model9', 'model8', 'model7', 'model6', 'model5', 'model4', 'model3', 'model2', 'model1', 'model0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select models specified in previous cell\n",
    "eval_calculations={}\n",
    "changeable_config={}\n",
    "\n",
    "if select_models:\n",
    "    for key in copy.deepcopy(different_plots):\n",
    "        current_task_data = different_plots[key]\n",
    "        eval_calculations[key] = {second_key: current_task_data[second_key] for second_key in models_you_want}\n",
    "            \n",
    "    for config_key in models_you_want:\n",
    "        if config_key != 'OLS':\n",
    "            changeable_config[config_key] = copy.deepcopy(configs)[config_key]\n",
    "else:\n",
    "    eval_calculations = copy.deepcopy(different_plots)\n",
    "    changeable_config = copy.deepcopy(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(eval_calculations['task0_data0'].keys())\n",
    "print(eval_calculations['task1_data0'].keys())\n",
    "print(changeable_config.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_dir=\"/Users/harmonbhasin/College/Research/huLab/paper_figures/task_mask\"\n",
    "figure_one_plot_results_horizontal(different_plots=eval_calculations, \n",
    "                        configs=changeable_config, \n",
    "                        task_args=task_args, \n",
    "                        data_args=data_args, \n",
    "                        type = 'task', \n",
    "                        head_dir=head_dir, \n",
    "                        smooth=True,\n",
    "                        width=3.3,\n",
    "                        height=1.9,\n",
    "                        name='lazier', \n",
    "                        title = 'Test Time MSE')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
